{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import files to create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import udf, col, lit, year, month, upper, to_date\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load configuration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Load cinfigerations\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to create Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "        creating the session\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to convert to date format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Convert_SAS_to_date(date):\n",
    "        if date is not None:\n",
    "            return pd.to_timedelta(date, unit='D') + pd.Timestamp('1960-1-1')\n",
    "    \n",
    "Convert_SAS_to_date_udf = udf(Convert_SAS_to_date, DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to get all .sas7bdat files from repository location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Function to get all filepaths from repository ending with '.sas7bdat'\n",
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.sas7bdat'))\n",
    "        for f in files :\n",
    "            all_files.append(os.path.abspath(f))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to proccess and create Tables from Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_immigration_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads immigration data from s3\n",
    "    and proccess data to get fact and dimension tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Apache Spark session\n",
    "    input_data : str\n",
    "        The path prefix for the immigration-data.\n",
    "    output_data : str\n",
    "        The path prefix for the output data.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # get filepath to immigration data files\n",
    "    filepath = input_data + '../../data/18-83510-I94-Data-2016/'\n",
    "    all_files = get_files(filepath)\n",
    "    \n",
    "    #create immigration-data schema\n",
    "    immigrationSchema = R([\n",
    "        Fld(\"cicid\",Int()),\n",
    "        Fld(\"i94yr\",Int()),\n",
    "        Fld(\"i94mon\",Int()),\n",
    "        Fld(\"i94cit\",Int()),\n",
    "        Fld(\"i94res\",Int()),\n",
    "        Fld(\"i94port\",Str()),\n",
    "        Fld(\"arrdate\",Dbl()),\n",
    "        Fld(\"i94mode\",Int()),\n",
    "        Fld(\"i94addr\",Str()),\n",
    "        Fld(\"depdate\",Dbl()),\n",
    "        Fld(\"i94bir\",Int()),\n",
    "        Fld(\"i94visa\",Int()),\n",
    "        Fld(\"count\",Int()),\n",
    "        Fld(\"dtadfile\",Str()),\n",
    "        Fld(\"visapost\",Str()),\n",
    "        Fld(\"occup\",Str()),\n",
    "        Fld(\"entdepa\",Str()),\n",
    "        Fld(\"entdepd\",Str()),\n",
    "        Fld(\"entdepu\",Str()),\n",
    "        Fld(\"matflag\",Str()),\n",
    "        Fld(\"biryear\",Int()),\n",
    "        Fld(\"dtaddto\",Str()),\n",
    "        Fld(\"gender\",Str()),\n",
    "        Fld(\"insnum\",Str()),\n",
    "        Fld(\"airline\",Str()),\n",
    "        Fld(\"admnum\",DecimalType(18, 0)),\n",
    "        Fld(\"fltno\",Str()),\n",
    "        Fld(\"visatype\",Str())\n",
    "    ])\n",
    "    \n",
    "    # Create an empty RDD\n",
    "    emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Create an empty RDD with expected schema\n",
    "    df_immigration = spark.createDataFrame(data = emp_RDD,schema = immigrationSchema)\n",
    "    \n",
    "    # iterate over files and process\n",
    "    for i, datafile in enumerate(all_files, 1):\n",
    "        # read files in immigration data files\n",
    "        df_temp = spark.read.format('com.github.saurfang.sas.spark').load(datafile)\n",
    "        df_temp = df_temp\\\n",
    "                            .withColumn('cicid', col('cicid').cast(IntegerType()))\\\n",
    "                            .withColumn('i94yr', col('i94yr').cast(IntegerType()))\\\n",
    "                            .withColumn('i94mon', col('i94mon').cast(IntegerType()))\\\n",
    "                            .withColumn('i94cit', col('i94cit').cast(IntegerType()))\\\n",
    "                            .withColumn('i94res', col('i94res').cast(IntegerType()))\\\n",
    "                            .withColumn('i94mode', col('i94mode').cast(IntegerType()))\\\n",
    "                            .withColumn('i94visa', col('i94visa').cast(IntegerType()))\\\n",
    "                            .withColumn('count', col('count').cast(IntegerType()))\\\n",
    "                            .withColumn('biryear', col('biryear').cast(IntegerType()))\\\n",
    "                            .withColumn('i94bir', col('i94bir').cast(IntegerType()))\\\n",
    "                            .withColumn('admnum', col('admnum').cast(DecimalType(18, 0)))\n",
    "        \n",
    "        if datafile == all_files[4]:\n",
    "            df_temp = df_temp\\\n",
    "                                .drop('validres','delete_days','delete_mexl','delete_dup','delete_visa','delete_recdup')\n",
    "        df_immigration = df_immigration.union(df_temp)   \n",
    "            \n",
    "            \n",
    "    # Create Fact Table: Immigrations\n",
    "    Immigrations = df_immigration.select('cicid', 'i94yr', 'i94mon','i94addr', 'i94port', 'i94mode','i94visa', 'arrdate', 'depdate' ,'matflag')\\\n",
    "                    .distinct()\\\n",
    "                    .withColumn(\"immigration_id\", monotonically_increasing_id())\n",
    "            \n",
    "            \n",
    "    # Rename Columns in Fact Table\n",
    "    new_column_name = ['cic_id', 'year', 'month', 'state_code','port_code', 'mode_code','visa_code',\\\n",
    "                               'arrival_date','departure_date','match_flag','immigration_id']\n",
    "    Immigrations = Immigrations.toDF(*new_column_name)\n",
    "    \n",
    "    \n",
    "    # convert both arrival and departure date to date format\n",
    "    Immigrations = Immigrations.withColumn('arrival_date',Convert_SAS_to_date_udf(Immigrations['arrival_date']))\n",
    "    Immigrations = Immigrations.withColumn('departure_date',Convert_SAS_to_date_udf(Immigrations['departure_date']))\n",
    "    \n",
    "    # write Immigrations table to parquet files partitioned by state_code\n",
    "    Immigrations.write.mode('overwrite').partitionBy(\"state_code\").parquet(output_data+'Immigrations/')\n",
    "    \n",
    "    \n",
    "    #Create 1st Dimension Table: Immigrants\n",
    "    Immigrants = df_immigration.select('cicid', 'i94cit', 'i94res', 'i94bir', 'gender', 'insnum')\\\n",
    "                     .distinct()\\\n",
    "                     .withColumn(\"immigrants_id\", monotonically_increasing_id())\n",
    "    \n",
    "    \n",
    "    # Rename Columns for 1st Dimension Table: Immigrants\n",
    "    new_column_name = ['cic_id','citizen_country', 'residence_country','age','gender','ins_num','immigrants_id']\n",
    "    Immigrants = Immigrants.toDF(*new_column_name)\n",
    "    \n",
    "    \n",
    "    # write Immigrants table to parquet files \n",
    "    Immigrants.write.mode('overwrite').parquet(output_data+'Immigrants/')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Create 2nd Dimension Table: Airports\n",
    "    Airports = df_immigration.select('cicid','airline','fltno','admnum','visatype')\\\n",
    "                    .distinct()\\\n",
    "                    .withColumn(\"Airports_id\", monotonically_increasing_id())\n",
    "    \n",
    "    \n",
    "    #Rename Columns 2nd Dimension Table: Airports\n",
    "    new_column_name = ['cic_id','airline','flight_number','admin_number','visa_type','Airports_id']\n",
    "    Airports = Airports.toDF(*new_column_name)\n",
    "    \n",
    "    # write Immigrants table to parquet files \n",
    "    Airports.write.mode('overwrite').parquet(output_data+'Airports/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to proccess and create Tables from Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_demographic_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads demographic data from s3\n",
    "    and proccess data to get dimension tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Apache Spark session\n",
    "    input_data : str\n",
    "        The path prefix for the demographic-data.\n",
    "    output_data : str\n",
    "        The path prefix for the output data.\n",
    "    \n",
    "    \"\"\"\n",
    "    # get filepath to demographic data file\n",
    "    demographic_data = input_data + \"us-cities-demographics.csv\"\n",
    "    \n",
    "\n",
    "     # read demographic data file\n",
    "    df_demographic = spark.read.format('csv').load(demographic_data,delimiter=';',header=True)\n",
    "    df_demographic = df_demographic\\\n",
    "                    .withColumn('City',upper(df_demographic['city']))\\\n",
    "                    .withColumn('State',upper(df_demographic['State']))\\\n",
    "                    .withColumn('Male Population', col('Male Population').cast(IntegerType()))\\\n",
    "                    .withColumn('Female Population', col('Female Population').cast(IntegerType()))\\\n",
    "                    .withColumn('Total Population', col('Total Population').cast(IntegerType()))\\\n",
    "                    .withColumn('Number of Veterans', col('Number of Veterans').cast(IntegerType()))\\\n",
    "                    .withColumn('Foreign-born', col('Foreign-born').cast(IntegerType()))\\\n",
    "                    .withColumn('Median Age', col('Median Age').cast(DoubleType()))\\\n",
    "                    .withColumn('Average Household Size', col('Average Household Size').cast(DoubleType()))\n",
    "                    \n",
    "                    \n",
    "    \n",
    "    \n",
    "    #Create Dimension Table: Populations\n",
    "    Populations = df_demographic.select('City','State','State Code','Male Population','Female Population',\\\n",
    "                                        'Total Population','Number of Veterans','Foreign-born','Race')\\\n",
    "                    .distinct()\\\n",
    "                    .withColumn(\"population_id\", monotonically_increasing_id())\n",
    "    \n",
    "    #Rename Columns for Dimension Table: Populations\n",
    "    new_column_name =['city','state','state_code','male_population','female_population','total_population'\\\n",
    "                      ,'num_of_veterans','foreign_born','race','population_id']\n",
    "    Populations = Populations.toDF(*new_column_name)\n",
    "    \n",
    "    # write Populations table to parquet files partitioned by state\n",
    "    Populations.write.mode('overwrite').partitionBy(\"state\").parquet(output_data+'Populations/')\n",
    "    \n",
    "    \n",
    "    #Create 2nd Dimension Table: Population_Statistics\n",
    "    Population_Statistics = df_demographic.select('City','State','State Code','Median Age','Average Household Size')\\\n",
    "                    .distinct()\\\n",
    "                    .withColumn(\"pop_statistics_id\", monotonically_increasing_id())\n",
    "    \n",
    "    #Rename Columns for 2nd Dimension Table: Population_Statistics\n",
    "    new_column_name = ['city','state','state_code','median_age','avg_household_size','pop_statistics_id']\n",
    "    Population_Statistics =  Population_Statistics.toDF(*new_column_name)\n",
    "    \n",
    "    # write Population_Statistics table to parquet files partitioned by state\n",
    "    Population_Statistics.write.mode('overwrite').partitionBy(\"state\").parquet(output_data+'Population_Statistics/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to proccess and create Tables from Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_temperature_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads temperature data from s3\n",
    "    and proccess data to get dimension tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Apache Spark session\n",
    "    input_data : str\n",
    "        The path prefix for the temperature-data.\n",
    "    output_data : str\n",
    "        The path prefix for the output data.\n",
    "    \n",
    "    \"\"\"\n",
    "    # get filepath to temperature data file\n",
    "    temperature_data = input_data + \"../../data2/*.csv\"\n",
    "    \n",
    "    \n",
    "     # read temperature data file\n",
    "    df_temperature = spark.read.format('csv').load(temperature_data,header=True)\n",
    "    df_temperature = df_temperature\\\n",
    "                        .withColumn('AverageTemperature', col('AverageTemperature').cast(DoubleType()))\\\n",
    "                        .withColumn('AverageTemperatureUncertainty', col('AverageTemperatureUncertainty').cast(DoubleType()))\n",
    "                        .withColumn('City',upper(df_temperature['City']))\\\n",
    "                        .withColumn('Country',upper(df_temperature['Country']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add year and month to temperature data\n",
    "    df_temperature = df_temperature.withColumn('dt', to_date(df_temperature['dt']))\n",
    "    df_temperature = df_temperature.withColumn('year', year(df_temperature['dt']))\n",
    "    df_temperature = df_temperature.withColumn('month', month(df_temperature['dt']))\n",
    "    \n",
    "    \n",
    "       \n",
    "    \n",
    "    #Create Dimension Table: Temperatures\n",
    "    Temperatures = df_temperature.select('dt','year','month','Country','City','Latitude','Longitude').distinct()\n",
    "    \n",
    "    #Rename Columns\n",
    "    new_column_name =['date','year','month','country','city','latitude','longitude']\n",
    "    Temperatures = Temperatures.toDF(*new_column_name)\n",
    "    \n",
    "    # write Temperatures table to parquet files \n",
    "    Temperatures.write.mode('overwrite').parquet(output_data+'Temperatures/')\n",
    "    \n",
    "    \n",
    "    #Create Dimension Table Temperature_Statistics\n",
    "    Temperature_Statistics = df_temperature.select('dt','year','month','Country','City'\\\n",
    "                                                       ,'AverageTemperature','AverageTemperatureUncertainty')\n",
    "    \n",
    "    \n",
    "    #Rename Columns\n",
    "    new_column_name =['date','year','month','country','city','avg_temp','avg_temp_uncertainty']\n",
    "    Temperature_Statistics = Temperature_Statistics.toDF(*new_column_name)\n",
    "    \n",
    "    \n",
    "    # write Temperatures table to parquet files \n",
    "    Temperature_Statistics.write.mode('overwrite').parquet(output_data+'Temperature_Statistics/')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function to proccess and create Tables from Library Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_libray_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    Loads libray data from s3\n",
    "    and proccess data to get dimension tables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    spark : SparkSession\n",
    "        Apache Spark session\n",
    "    input_data : str\n",
    "        The path prefix for the libray-data.\n",
    "    output_data : str\n",
    "        The path prefix for the output data.\n",
    "    \n",
    "    \"\"\"\n",
    "    # get filepath to library data file\n",
    "    library_data = input_data + \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "    \n",
    "    \n",
    "     # read library data file\n",
    "    with open(library_data) as library:\n",
    "        lines = library.readlines()\n",
    "    \n",
    "    #Create Countries Auxilary table\n",
    "    \n",
    "    # Create Auxilary Table: Countries\n",
    "    # Create an empty RDD\n",
    "    emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Create an expected schema\n",
    "    columns = StructType([StructField('code',StringType(), True)\\\n",
    "                    ,StructField('country',StringType(), True)])\n",
    "    # Create an empty RDD with expected schema\n",
    "    Countries = spark.createDataFrame(data = emp_RDD,\n",
    "                           schema = columns)\n",
    "    \n",
    "    # Insert into Auxilary Table: Countries\n",
    "    country_data = lines[9:298]\n",
    "    for data in country_data:\n",
    "        temp = data.split('=')\n",
    "        list = [temp[0].strip(), temp[1].strip().strip(\"'\").strip(\";\")]\n",
    "        columns =  ['code', 'country']\n",
    "        newRow = spark.createDataFrame([list], columns)\n",
    "        Countries = Countries.union(newRow)\n",
    "    \n",
    "    # Convert code datatype to int\n",
    "    Countries = Countries.withColumn('code', col('code').cast(IntegerType()))\n",
    "    \n",
    "    # write Countries Auxilary Table to parquet files \n",
    "    Countries.write.mode('overwrite').parquet(output_data+'Countries/')\n",
    "    \n",
    "        \n",
    "    # Create Auxilary Table: States\n",
    "    # Create an empty RDD\n",
    "    emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Create an expected schema\n",
    "    columns = StructType([StructField('state_code',StringType(), True)\\\n",
    "                    ,StructField('state',StringType(), True)])\n",
    "    # Create an empty RDD with expected schema\n",
    "    States = spark.createDataFrame(data = emp_RDD,schema = columns)\n",
    "    \n",
    "    # Insert into Auxilary Table: States\n",
    "    country_data = lines[981:1036]\n",
    "    for data in country_data:\n",
    "        temp = data.split('=')\n",
    "        list = [temp[0].strip(), temp[1].strip().strip(\"'\").strip(\";\")]\n",
    "        columns =  ['state_code', 'state']\n",
    "        newRow = spark.createDataFrame([list], columns)\n",
    "        States = States.union(newRow)\n",
    "        \n",
    "    # write States Auxilary Table to parquet files \n",
    "    States.write.mode('overwrite').parquet(output_data+'States/')\n",
    "    \n",
    "    \n",
    "    # Create Auxilary Table: Ports\n",
    "    # Create an empty RDD\n",
    "    emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Create an expected schema\n",
    "    columns = StructType([StructField('port_code',StringType(), True)\\\n",
    "                    ,StructField('port_city',StringType(), True)])\n",
    "    # Create an empty RDD with expected schema\n",
    "    Ports = spark.createDataFrame(data = emp_RDD,schema = columns)\n",
    "    \n",
    "    # Insert into Auxilary Table: Ports\n",
    "    country_data = lines[302:961]\n",
    "    for data in country_data:\n",
    "        temp = data.split('=')\n",
    "        list = [temp[0].strip(), temp[1].strip().strip(\"'\").split(',')[0]]\n",
    "        columns =  ['port_code', 'port_city']\n",
    "        newRow = spark.createDataFrame([list], columns)\n",
    "        Ports = Ports.union(newRow)\n",
    "        \n",
    "     # write Ports Auxilary Table to parquet files \n",
    "    Ports.write.mode('overwrite').parquet(output_data+'Ports/')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create Auxilary Table: Visas\n",
    "    # Create an empty RDD\n",
    "    emp_RDD = spark.sparkContext.emptyRDD()\n",
    "\n",
    "    # Create an expected schema\n",
    "    columns = StructType([StructField('visa_code',StringType(), True)\\\n",
    "                    ,StructField('visa',StringType(), True)])\n",
    "\n",
    "    # Create an empty RDD with expected schema\n",
    "    Visas = spark.createDataFrame(data = emp_RDD,schema = columns)\n",
    "    \n",
    "    # Insert into Auxilary Table: Visas\n",
    "    country_data = lines[1046:1049]\n",
    "    for data in country_data:\n",
    "        temp = data.split('=')\n",
    "        list = [temp[0].strip(), temp[1].strip().strip(\"'\").strip(\";\")]\n",
    "        columns =  ['visa_code', 'visa']\n",
    "        newRow = spark.createDataFrame([list], columns)\n",
    "        Visas = Visas.union(newRow)\n",
    "    \n",
    "    # Convert visa_code datatype to int\n",
    "    Visas = Visas.withColumn('visa_code', col('visa_code').cast(IntegerType()))\n",
    "    \n",
    "    # write Visas Auxilary Table to parquet files \n",
    "    Visas.write.mode('overwrite').parquet(output_data+'Visas/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Define function for the main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"\"\n",
    "    \n",
    "    output_data = \"Capstone_Project/\"\n",
    "    \n",
    "    \n",
    "    process_demographic_data(spark, input_data, output_data)\n",
    "    process_temperature_data(spark, input_data, output_data)\n",
    "    process_libray_data(spark, input_data, output_data)\n",
    "    process_immigration_data(spark, input_data, output_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Run main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
